{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923261bb",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9457a03a-5d54-435a-a61e-e5bed06c59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/tripadvisor_hotel_reviews.csv\")\n",
    "review_texts = list(data.Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f90754-f524-4464-bf11-c4d3473bb91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract candidate 1-grams and 2-grams \n",
    "n_gram_range = (1, 2)\n",
    "vectorizer = CountVectorizer(ngram_range=n_gram_range, stop_words=stopwords.words('english'))\n",
    "vectorizer.fit(review_texts)\n",
    "candidates = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get noun phrases and nouns from reviews\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "all_nouns = set()\n",
    "\n",
    "for doc in review_texts:\n",
    "    doc_processed = nlp(doc)\n",
    "    # Add noun chunks\n",
    "    all_nouns.add(chunk.text.strip().lower() for chunk in doc_processed.noun_chunks)\n",
    "    # Add nouns\n",
    "    for token in doc_processed:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                all_nouns.add(token.text)\n",
    "\n",
    "# Filter candidate topics to only those in the nouns set\n",
    "candidates = [c for c in candidates if c in all_nouns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f27873-756a-4609-beb8-6d814f7a88a9",
   "metadata": {},
   "source": [
    "## Embed candidates and documents and find matching topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd916313-13fd-4fac-b59b-0084570bc2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_topics(documents, candidates, num_topics):\n",
    "    #model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "    # Encode each of the articles\n",
    "    doc_embeddings = [model.encode([doc]) for doc in documents]\n",
    "    # Encode the candidate topics\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    # Calculate cosine similarity between each document and candidate topics\n",
    "    # Take the top candidate topics as keywords for each document\n",
    "    review_keywords = []\n",
    "    for doc in doc_embeddings:\n",
    "        scores = cosine_similarity(doc, candidate_embeddings)\n",
    "        keywords = [candidates[index] for index in scores.argsort()[0][-num_topics:]]\n",
    "        review_keywords.append(keywords)\n",
    "    \n",
    "    return review_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083dbbad-1c45-42ad-b2ed-17b1a850fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model_topics(review_texts, candidates, num_topics=5)\n",
    "\n",
    "data[\"Topic Keywords\"] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73454c30-8816-46c2-bc00-f3180df77b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,keywords in enumerate(topics[:10]):\n",
    "    print(review_texts[i])\n",
    "    print('Topic keywords: {}'.format(keywords))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb7376-6bdd-4154-9823-c293e922c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"topics_from_transformer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bdd3c7-4b5d-4348-9651-da37187751bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc799f6b-49c9-46a9-95e6-3b454d23c0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30bee6-a670-427b-b341-349e7d40e1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa9d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f61f47c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b659e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd5430ea-df24-49a8-bb65-f00692d18ff5",
   "metadata": {},
   "source": [
    "## Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd864e-276b-4067-8125-e65cd857969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "txt = review_texts[1]\n",
    "doc = nlp(txt)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e750a-fc51-4458-ae6a-b7a60b94e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    out = {}\n",
    "    noun = chunk.root\n",
    "    if noun.pos_ != 'NOUN':\n",
    "        continue\n",
    "    out['noun'] = noun\n",
    "    for tok in chunk:\n",
    "        if tok != noun:\n",
    "            out[tok.pos_] = tok\n",
    "    chunks.append(out)\n",
    "    \n",
    "chunks = [chunk for chunk in chunks if 'ADJ' in chunk.keys()]\n",
    "    \n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb41eb-c02c-4f79-aefd-dd577ef4b136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d10d16d6-bf05-4893-82f2-109696d37d04",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090600bb-ace6-4783-b0ea-685aa41592c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a26e62-7ee1-4cfc-93e1-344adfaa12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/tripadvisor_hotel_reviews.csv\")\n",
    "review_list = list(data.Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4df9c1-98f6-4678-95d4-d872d25d755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_to_sentiment(rating):\n",
    "    if rating>3 and rating<=5:\n",
    "        return 2\n",
    "    elif rating == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['Sentiment'] = data['Rating'].apply(rating_to_sentiment)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da9169-3036-4a18-864a-3878b295665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create numeircal features based on\n",
    "n_gram_range = (1, 1)\n",
    "vectorizer = CountVectorizer(ngram_range=n_gram_range, stop_words=stopwords.words('english'))\n",
    "\n",
    "#With respect to the memory issues. That is something that I hopefully will have f\n",
    "#ixed in the new version. However, you can reduce the sparse matrix's size quite \n",
    "#a bit by setting min_df=10 or higher in the vectorizer. This will reduce the number \n",
    "#of words significantly which results in less sparsity.\n",
    "\n",
    "\n",
    "#X = vectorizer.fit_transform(review_list).toarray()\n",
    "Y = np.array(data.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac16d7-4fa7-48d1-9979-03f36824139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(review_list, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6f5f1-47a6-4b53-8a95-a488bb6046d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3700f-a742-42b5-838b-f77517769cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025b451-f72b-4373-ba40-21e96a15619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4991f-6a2c-4098-a0bc-d82fe04753b1",
   "metadata": {},
   "source": [
    "## Shap explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9bee5-2043-4db5-8b2e-15294e17ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f3880-3f43-4adb-8d43-72dc776a3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_shap_values = shap.KernelExplainer(clf.predict, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a2b7f-61c9-44a4-a6b2-923ebd181061",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f9d46-d9ba-4772-bdef-e65e454cfaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(rf_shap_values, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96894edb-7f81-4cd2-97cd-d0613c6acd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef4b1e-7d0a-40ff-af83-26b3e33ff498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55347999-047b-4d65-896c-207887c3c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03f088-8995-4152-83e7-ef77191b45cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(X_test[0].reshape(1,-1), columns = list(vectorizer.get_feature_names_out()))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75cff4-537b-4fa4-8963-37599528857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Shap values\n",
    "choosen_instance = d\n",
    "shap_values = explainer.shap_values(choosen_instance, check_additivity=False)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5306cc04-8a0c-4a08-b82c-0368ac9648fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd40f2-8b88-4acf-9ada-54bb291e5374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eea6fa-475f-400e-80c4-7b0689dbdaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(vectorizer.transform(review_list[0]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705bd44b-804a-4d1c-83ce-e780742b556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcabcf8-5a6e-4c39-bd47-adc31594bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1829f5-9897-45e5-90d7-7d0c5e359cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.transform([\"My name is archit\"]).toarray()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipi540",
   "language": "python",
   "name": "aipi540"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
