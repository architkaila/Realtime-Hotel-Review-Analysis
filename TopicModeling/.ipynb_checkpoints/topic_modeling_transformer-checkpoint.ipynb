{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923261bb",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc8c4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/aipi540-s23/ak704/miniconda3/envs/aipi540/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "import texthero as hero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9457a03a-5d54-435a-a61e-e5bed06c59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/tripadvisor_hotel_reviews.csv\")\n",
    "review_texts = list(data.Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f90754-f524-4464-bf11-c4d3473bb91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract candidate 1-grams and 2-grams \n",
    "n_gram_range = (1, 2)\n",
    "vectorizer = CountVectorizer(ngram_range=n_gram_range, stop_words=stopwords.words('english'))\n",
    "vectorizer.fit(review_texts)\n",
    "candidates = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get noun phrases and nouns from reviews\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "all_nouns = set()\n",
    "\n",
    "for doc in review_texts:\n",
    "    doc_processed = nlp(doc)\n",
    "    # Add noun chunks\n",
    "    all_nouns.add(chunk.text.strip().lower() for chunk in doc_processed.noun_chunks)\n",
    "    # Add nouns\n",
    "    for token in doc_processed:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                all_nouns.add(token.text)\n",
    "\n",
    "# Filter candidate topics to only those in the nouns set\n",
    "candidates = [c for c in candidates if c in all_nouns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f27873-756a-4609-beb8-6d814f7a88a9",
   "metadata": {},
   "source": [
    "## Embed candidates and documents and find matching topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd916313-13fd-4fac-b59b-0084570bc2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_topics(documents, candidates, num_topics):\n",
    "    #model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "    # Encode each of the articles\n",
    "    doc_embeddings = [model.encode([doc]) for doc in documents]\n",
    "    # Encode the candidate topics\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    # Calculate cosine similarity between each document and candidate topics\n",
    "    # Take the top candidate topics as keywords for each document\n",
    "    review_keywords = []\n",
    "    for doc in doc_embeddings:\n",
    "        scores = cosine_similarity(doc, candidate_embeddings)\n",
    "        keywords = [candidates[index] for index in scores.argsort()[0][-num_topics:]]\n",
    "        review_keywords.append(keywords)\n",
    "    \n",
    "    return review_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083dbbad-1c45-42ad-b2ed-17b1a850fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model_topics(review_texts, candidates, num_topics=5)\n",
    "\n",
    "data[\"Topic Keywords\"] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73454c30-8816-46c2-bc00-f3180df77b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,keywords in enumerate(topics[:10]):\n",
    "    print(review_texts[i])\n",
    "    print('Topic keywords: {}'.format(keywords))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb7376-6bdd-4154-9823-c293e922c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"topics_from_transformer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bdd3c7-4b5d-4348-9651-da37187751bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc799f6b-49c9-46a9-95e6-3b454d23c0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30bee6-a670-427b-b341-349e7d40e1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa9d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f61f47c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b659e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd5430ea-df24-49a8-bb65-f00692d18ff5",
   "metadata": {},
   "source": [
    "## Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd864e-276b-4067-8125-e65cd857969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "txt = abc\n",
    "doc = nlp(txt)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e750a-fc51-4458-ae6a-b7a60b94e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    out = {}\n",
    "    noun = chunk.root\n",
    "    if noun.pos_ != 'NOUN':\n",
    "        continue\n",
    "    out['noun'] = noun\n",
    "    for tok in chunk:\n",
    "        if tok != noun:\n",
    "            out[tok.pos_] = tok\n",
    "    chunks.append(out)\n",
    "    \n",
    "chunks = [chunk for chunk in chunks if 'ADJ' in chunk.keys()]\n",
    "    \n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb41eb-c02c-4f79-aefd-dd577ef4b136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d10d16d6-bf05-4893-82f2-109696d37d04",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090600bb-ace6-4783-b0ea-685aa41592c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\", enable=[\"tokenizer\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a26e62-7ee1-4cfc-93e1-344adfaa12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/tripadvisor_hotel_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774605c3-1ad3-42ba-9e7c-11ff7938f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4df9c1-98f6-4678-95d4-d872d25d755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_to_sentiment(rating):\n",
    "    if rating>4:\n",
    "        return 1 # pos\n",
    "    elif rating < 2:\n",
    "        return 0 # neu\n",
    "    #else:\n",
    "        #return 0 # neg\n",
    "\n",
    "data['Sentiment'] = data['Rating'].apply(rating_to_sentiment)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f85d67-bf91-441f-9dab-92e972864001",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e82eff-a58f-4f99-bb1d-c0614b4d6149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence,method='spacy'):\n",
    "# Tokenize and lemmatize text, remove stopwords and punctuation\n",
    "\n",
    "    punctuations = string.punctuation\n",
    "    stopwords = list(STOP_WORDS)\n",
    "\n",
    "    if method=='nltk':\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(sentence,preserve_line=True)\n",
    "        # Remove stopwords and punctuation\n",
    "        tokens = [word for word in tokens if word not in stopwords and word not in punctuations]\n",
    "        # Lemmatize\n",
    "        #wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        #tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "        tokens = \" \".join([i for i in tokens])\n",
    "    else:\n",
    "        # Tokenize\n",
    "        #with nlp.select_pipes(enable=['tokenizer','lemmatizer']):\n",
    "        tokens = nlp(sentence)\n",
    "        # Lemmatize\n",
    "        tokens = [word.lemma_.lower().strip() for word in tokens]\n",
    "        # Remove stopwords and punctuation\n",
    "        #tokens = [word for word in tokens if word not in stopwords and word not in punctuations]\n",
    "        tokens = \" \".join([i for i in tokens])\n",
    "    return tokens\n",
    "\n",
    "# def clean(review):\n",
    "    \n",
    "#     review = review.lower()\n",
    "#     review = re.sub('[^a-z A-Z 0-9-]+', '', review)\n",
    "#     #review = \" \".join([word for word in review.split() if word not in stopwords.words('english')])\n",
    "    \n",
    "#     return review\n",
    "\n",
    "# tqdm.pandas()\n",
    "# data['Review'] = data['Review'].progress_apply(clean)\n",
    "# data.head()\n",
    "# # Process the reviews\n",
    "# #tqdm.pandas()\n",
    "# #df_term_freq['processed_reviews'] = df_term_freq['Review'].progress_apply(lambda x: tokenize(x,method='nltk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e9772-3fef-42c2-941d-6efcd299f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_reviews'] = data['Review'].pipe(hero.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec75779-1f61-4c93-a201-e75bd2e1ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "data['cleaned_reviews'] = data['cleaned_reviews'].progress_apply(lambda x: tokenize(x,method='spacy'))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da9169-3036-4a18-864a-3878b295665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create numeircal features based on\n",
    "n_gram_range = (1, 1)\n",
    "#vectorizer = CountVectorizer(ngram_range=n_gram_range, stop_words=stopwords.words('english'), min_df=10 )\n",
    "\n",
    "# you can reduce the sparse matrix's size quite \n",
    "# a bit by setting min_df=10 or higher in the vectorizer. This will reduce the number \n",
    "# of words significantly which results in less sparsity.\n",
    "\n",
    "\n",
    "#X = vectorizer.fit_transform(review_list).toarray()\n",
    "#X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "#X_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "Y = np.array(data.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb88bb2-05c9-45a3-a0e1-cec8eb2e9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(train_data, test_data, ngram_range, method='count'):\n",
    "    if method == 'tfidf':\n",
    "        # Create features using TFIDF\n",
    "        vec = TfidfVectorizer(ngram_range=ngram_range, min_df=500)\n",
    "        X_train = vec.fit_transform(train_data)\n",
    "        X_test = vec.transform(test_data)\n",
    "\n",
    "    else:\n",
    "        # Create features using word counts\n",
    "        vec = CountVectorizer(ngram_range=ngram_range, min_df=1000)\n",
    "        X_train = vec.fit_transform(train_data)\n",
    "        X_test = vec.transform(test_data)\n",
    "\n",
    "    return X_train, X_test, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac16d7-4fa7-48d1-9979-03f36824139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.cleaned_reviews.values, data.Sentiment.values, test_size=0.2, random_state=0, stratify=data.Sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42e3bf-c954-4943-ae5e-ed4c31c09c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = X_test.copy()\n",
    "print(abc[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6f5f1-47a6-4b53-8a95-a488bb6046d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, vec = build_features(X_train, X_test, n_gram_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac6f27-617c-4658-8e31-a0e23de13622",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3700f-a742-42b5-838b-f77517769cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(class_weight={0:1, 1:6})\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025b451-f72b-4373-ba40-21e96a15619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cc525-6bb4-45aa-b003-46c3ba07bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffaad64-96e4-4a65-9579-d959163706c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4991f-6a2c-4098-a0bc-d82fe04753b1",
   "metadata": {},
   "source": [
    "## Shap explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9bee5-2043-4db5-8b2e-15294e17ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef4b1e-7d0a-40ff-af83-26b3e33ff498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tree Explainer object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93301b5a-1988-4080-917d-387718b826a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d0775-3159-4d85-86e0-a294368378bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test.toarray()[instance].reshape(1, -1), check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87221175-fd7e-42f2-908e-09789e9712e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d3493-0ea2-4f3d-8dd3-04df534fbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abc[instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6f069-124b-4e59-bddd-7c06df33571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d1fba-019a-4f05-a0c4-00b031a7674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(vec.get_feature_names_out())[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd552b0-fa55-4de3-aa49-e3e9a96d47d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0], shap_values[0], feature_names=list(vec.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5195ecc-554a-4e87-8ba9-4c62bc66d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1], shap_values[1], feature_names=list(vec.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a784b6-3477-4bec-a07e-13f0ce347d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e87c91-dfe5-4219-a98a-8db9edca2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "choosen_instance = d\n",
    "shap_values = explainer.shap_values(choosen_instance, check_additivity=False)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance, check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75cff4-537b-4fa4-8963-37599528857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Shap values\n",
    "choosen_instance = d\n",
    "shap_values = explainer.shap_values(choosen_instance, check_additivity=False)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5306cc04-8a0c-4a08-b82c-0368ac9648fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd40f2-8b88-4acf-9ada-54bb291e5374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eea6fa-475f-400e-80c4-7b0689dbdaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(vectorizer.transform(review_list[0]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705bd44b-804a-4d1c-83ce-e780742b556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcabcf8-5a6e-4c39-bd47-adc31594bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1829f5-9897-45e5-90d7-7d0c5e359cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.transform([\"My name is archit\"]).toarray()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipi540",
   "language": "python",
   "name": "aipi540"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
