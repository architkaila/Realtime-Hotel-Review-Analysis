{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923261bb",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "import texthero as hero\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\", enable=[\"tokenizer\", \"lemmatizer\"])\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "import stanza\n",
    "# download and load the English model\n",
    "#stanza.download(\"en\")\n",
    "nlp_stanza = stanza.Pipeline(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5430ea-df24-49a8-bb65-f00692d18ff5",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a26e62-7ee1-4cfc-93e1-344adfaa12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/tripadvisor_hotel_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774605c3-1ad3-42ba-9e7c-11ff7938f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f303306-409c-4986-8ce2-9eb075f68cb5",
   "metadata": {},
   "source": [
    "## Convert ratings to Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4df9c1-98f6-4678-95d4-d872d25d755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_to_sentiment(rating):\n",
    "    if rating>4:\n",
    "        return 1 # pos\n",
    "    elif rating < 2:\n",
    "        return 0 # neg\n",
    "\n",
    "data['Sentiment'] = data['Rating'].apply(rating_to_sentiment)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f85d67-bf91-441f-9dab-92e972864001",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb27c70-bbf4-4593-ae4a-62af414b2150",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e82eff-a58f-4f99-bb1d-c0614b4d6149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "# Tokenize and lemmatize text, remove stopwords and punctuation\n",
    "    punctuations = string.punctuation\n",
    "    stopwords = list(STOP_WORDS)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = nlp(sentence)\n",
    "    # Lemmatize\n",
    "    tokens = [word.lemma_.lower().strip() for word in tokens]\n",
    "    # Remove stopwords and punctuation\n",
    "    #tokens = [word for word in tokens if word not in stopwords and word not in punctuations]\n",
    "    tokens = \" \".join([i for i in tokens])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e9772-3fef-42c2-941d-6efcd299f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_reviews'] = data['Review'].pipe(hero.clean)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec75779-1f61-4c93-a201-e75bd2e1ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas() #Lematize\n",
    "data['cleaned_reviews'] = data['cleaned_reviews'].progress_apply(lambda x: tokenize(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c541402-afc3-4d75-80f3-9655f6894c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"../data/clean_lemmatized_data.pkl\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4f641-0473-4190-bb39-297c04fcf7eb",
   "metadata": {},
   "source": [
    "## Create Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da9169-3036-4a18-864a-3878b295665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create numeircal features based on\n",
    "n_gram_range = (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb88bb2-05c9-45a3-a0e1-cec8eb2e9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(train_data, test_data, ngram_range, method='count'):\n",
    "    if method == 'tfidf':\n",
    "        # Create features using TFIDF\n",
    "        vec = TfidfVectorizer(ngram_range=ngram_range, min_df=800)\n",
    "        X_train = vec.fit_transform(train_data.cleaned_Review)\n",
    "        X_test = vec.transform(test_data.cleaned_Review)\n",
    "\n",
    "    else:\n",
    "        # Create features using word counts\n",
    "        vec = CountVectorizer(ngram_range=ngram_range, min_df=800)\n",
    "        X_train = vec.fit_transform(train_data.cleaned_Review)\n",
    "        X_test = vec.transform(test_data.cleaned_Review)\n",
    "\n",
    "    return X_train, X_test, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac16d7-4fa7-48d1-9979-03f36824139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df, X_test_df, y_train, y_test = train_test_split(data.drop(columns=[\"Sentiment\", \"Rating\"]), data.Sentiment.values, test_size=0.2, random_state=0, stratify=data.Sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9405c-41ae-4007-af83-1d6a8316f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.reset_index(drop=True, inplace=True)\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8997c-a2cd-46d3-a354-e0c90543b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df.reset_index(drop=True, inplace=True)\n",
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6f5f1-47a6-4b53-8a95-a488bb6046d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec, X_test_vec, vec = build_features(X_train_df, X_test_df, n_gram_range, method='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac6f27-617c-4658-8e31-a0e23de13622",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_vec.shape, X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3700f-a742-42b5-838b-f77517769cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(class_weight={0:1, 1:6})\n",
    "clf.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025b451-f72b-4373-ba40-21e96a15619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test_vec)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cc525-6bb4-45aa-b003-46c3ba07bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = clf.predict_proba(X_test_vec)\n",
    "pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4991f-6a2c-4098-a0bc-d82fe04753b1",
   "metadata": {},
   "source": [
    "## Shap explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9bee5-2043-4db5-8b2e-15294e17ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef4b1e-7d0a-40ff-af83-26b3e33ff498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tree Explainer object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93301b5a-1988-4080-917d-387718b826a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d0775-3159-4d85-86e0-a294368378bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test_vec.toarray()[instance].reshape(1, -1), check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87221175-fd7e-42f2-908e-09789e9712e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d3493-0ea2-4f3d-8dd3-04df534fbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_df.loc[instance][\"Review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63835d2d-cc03-47fc-a712-c3b9f55b960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual Sentiment: \", y_test[instance])\n",
    "print(\"Predicted Sentiment: \", predictions[instance])\n",
    "print(\"Positive Sentiment Score: \", pred_prob[instance][1])\n",
    "print(\"Negative Sentiment Score: \", pred_prob[instance][0])\n",
    "\n",
    "pred_class = int(predictions[instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6f069-124b-4e59-bddd-7c06df33571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature index giving max shap value\n",
    "index = shap_values[pred_class].argmax()\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d1fba-019a-4f05-a0c4-00b031a7674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top adjective giving the max shap value\n",
    "top_adjective = list(vec.get_feature_names_out())[index]\n",
    "print(top_adjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc8ae6-bfd2-45c1-bdd3-00fcb6abbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Getting indices of N = 3 maximum values\n",
    "# x = np.argsort(scores)[::-1][:3]\n",
    "# print(\"Indices:\",x)\n",
    "candidates = list(vec.get_feature_names_out())\n",
    "keywords = [candidates[index] for index in shap_values[pred_class].argsort()[0][-4:]]\n",
    "keywords\n",
    "# # Getting N maximum values\n",
    "# print(\"Values:\",scores[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd552b0-fa55-4de3-aa49-e3e9a96d47d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[pred_class], shap_values[pred_class], feature_names=list(vec.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032bcc90-f380-426b-9622-6fc894b5d48f",
   "metadata": {},
   "source": [
    "## Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b809a13-296a-4a3f-927f-542bfa6bfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\"\n",
    "\n",
    "def grammar_correction(text):\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": f\"Can you correct the grammar of the following sentence? '{text}'\"}\n",
    "        ]\n",
    "    )\n",
    "    return dict(completion.choices[0].message)[\"content\"].replace(\"\\n\", \"\")\n",
    "\n",
    "review_text = grammar_correction(X_test_df.Review[instance])\n",
    "print(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a042c4-85c5-4427-a3fe-57486e612f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the sentence with stanza\n",
    "def extract_adjectives_with_nouns(text):\n",
    "    # extract all the adjectives and the nouns they are describing\n",
    "    doc = nlp_stanza(review_text)\n",
    "    adj_noun_pairs = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.upos == \"ADJ\":\n",
    "                for child in sentence.words:\n",
    "                    if child.head == word.id and child.upos == \"NOUN\":\n",
    "                        adj_noun_pairs.append((word.text, child.text))\n",
    "    return adj_noun_pairs\n",
    "\n",
    "def extract_adjectives_with_dependencies(text):\n",
    "    doc = nlp_stanza(text)\n",
    "    adjective_dependencies = {}  # Create an empty dictionary to store the adjective dependencies\n",
    "    for sent in doc.sentences:  # Loop through each sentence in the parsed document\n",
    "        for word in sent.words:  # Loop through each word in the sentence\n",
    "            if word.upos == 'ADJ':  # If the word is an adjective\n",
    "                adjective = word.text\n",
    "                parent_word = sent.words[word.head - 1]  # Get the parent word of the adjective\n",
    "                if parent_word.deprel == 'root':  # If the parent is the root of the tree, don't include it\n",
    "                    continue\n",
    "                dependencies = [parent_word.text]  # Initialize a list of dependencies with the parent word\n",
    "                for candidate_child in sent.words:\n",
    "                    if (candidate_child.head-1) == parent_word.id and candidate_child.deprel in ['amod', 'nsubj', 'advmod']:\n",
    "                        if candidate_child.text != parent_word.text:\n",
    "                            dependencies.append(candidate_child.text)  # Add the child to the list of dependencies\n",
    "                        for grandchild in sent.words:  # Loop through the children of the child\n",
    "                            if (grandchild.head-1) == candidate_child.id and grandchild.deprel in ['amod', 'nsubj', 'advmod']:\n",
    "                                if grandchild.text != candidate_child.text:\n",
    "                                    dependencies.append(grandchild.text)  # Add the grandchild to the list of dependencies\n",
    "                adjective_dependencies[adjective] = list(set(dependencies))  # Add the adjective and its dependencies to the dictionary, removing any duplicates\n",
    "    return adjective_dependencies\n",
    "\n",
    "adj_noun_pairs = extract_adjectives_with_nouns(review_text)\n",
    "adjective_dependencies = extract_adjectives_with_dependencies(review_text)\n",
    "\n",
    "# print the adjective-noun pairs\n",
    "top_adjective_shap = []\n",
    "remaining_adjectives = []\n",
    "\n",
    "for adj, noun in adj_noun_pairs:\n",
    "    if adj in keywords:\n",
    "        top_adjective_shap.append((adj, noun))\n",
    "    else:\n",
    "        remaining_adjectives.append((adj, noun))\n",
    "\n",
    "\n",
    "for adjective, dependencies in adjective_dependencies.items():\n",
    "    for dependency in dependencies:\n",
    "        if dependency!=\".\":\n",
    "            if adjective in keywords:\n",
    "                top_adjective_shap.append((adjective, noun))\n",
    "            else:\n",
    "                remaining_adjectives.append((adjective, noun))\n",
    "\n",
    "top_adjective_shap = list(set(top_adjective_shap))\n",
    "remaining_adjectives = list(set(remaining_adjectives))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Top Adjectives and Nouns using Shap Values: \\n\")\n",
    "for adj, noun in top_adjective_shap:\n",
    "    print(f\"{adj} -> {noun}\")\n",
    "\n",
    "print(\"\\nOther Adjectives and Nouns using Dependency Parsing: \\n\")\n",
    "for adj, noun in remaining_adjectives:\n",
    "    print(f\"{adj} -> {noun}\")\n",
    "\n",
    "## head - 1 we do for first sentence as it is \" the 0 index, but for all other sentence the zero index is .\"\n",
    "## check this bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e7a60-e24d-488c-86e7-d62db77e958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependency parsing using Spacy\n",
    "# #nlp = spacy.load('en_core_web_sm')\n",
    "# txt = X_test_df.Review[instance]\n",
    "# doc = nlp(txt)\n",
    "# doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536a9f5-37cc-4958-aed2-b73712ae5284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = []\n",
    "\n",
    "# for chunk in doc.noun_chunks:\n",
    "#     out = {}\n",
    "#     noun = chunk.root\n",
    "#     if noun.pos_ != 'NOUN':\n",
    "#         continue\n",
    "#     out['noun'] = noun\n",
    "#     for tok in chunk:\n",
    "#         if tok != noun:\n",
    "#             out[tok.pos_] = tok\n",
    "#     chunks.append(out)\n",
    "    \n",
    "# chunks = [chunk for chunk in chunks if 'ADJ' in chunk.keys()]\n",
    "    \n",
    "# print(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipi540_nlp_jupyter",
   "language": "python",
   "name": "aipi540_nlp_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
